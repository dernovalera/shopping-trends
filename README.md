Введение
В этой работе рассматривается простая нейронная сеть, которая предсказывает сумму покупки клиента интернет‑магазина по его характеристикам.​ Тема актуальна, потому что интернет‑магазинам важно заранее понимать, сколько примерно потратит каждый клиент, чтобы выдавать более точные рекомендации, планировать склад и маркетинговые акции.​ Наша цель – показать, что даже относительно простая нейросеть на табличных данных может давать хороший прогноз суммы покупки и служить основой для более сложных рекомендательных систем.​

Глава 1. Аналоги и существующие решения
Во многих крупных маркетплейсах уже используются модели, которые предсказывают поведение и «покупательскую способность» клиентов. Например, Wildberries и Ozon применяют методы машинного обучения для персональных рекомендаций, динамического ценообразования и подбора ассортимента под пользователя. В открытых материалах Ozon прямо указывает, что в рекомендательных системах используется градиентный бустинг деревьев и другие модели машинного обучения. В рамках работы рассматривается упрощённый, открытый пример нейросетевой модели. ​

Глава 2. Подробное описание реализации 
2.1. Архитектура нейросети 
Для задачи выбрана Sequential Fully Connected Network (FCN) из Keras — это последовательность плотных (Dense) слоёв, где каждый нейрон слоя связан со всеми нейронами предыдущего слоя.​
Архитектура:
Вход: 18 нейронов – по числу признаков клиента.
Скрытый слой 1: 64 нейрона.
Скрытый слой 2: 32 нейрона.
Выходной слой: 1 нейрон – предсказываем сумму покупки в долларах.​
Схема: 18 → 64 → 32 → 1, всего около 3345 обучаемых параметров (весов и смещений).​

2.2. Гиперпараметры и функции 
Функции активации
В скрытых слоях используется ReLU – если значение положительное, нейрон его пропускает, если отрицательное – обнуляет. Это ускоряет обучение и помогает модели лучше работать на табличных данных.​
На выходном слое стоит линейная (Linear) активация, так как нужно предсказывать реальное число (сумму в долларах), а не класс.​
Оптимизатор
Используется Adam с шагом обучения LR = 0.001. Это популярный адаптивный алгоритм, который сам подбирает величину шага при обновлении весов и ускоряет сходимость по сравнению с классическим градиентным спуском.​
Функция потерь
Для обучения выбрана MSE (Mean Squared Error) – среднеквадратичная ошибка. Она сильнее наказывает большие ошибки и хорошо подходит для регрессии, когда важны большие отклонения.​
Режим обучения
Количество эпох: 150 (максимум).
Размер батча: 32 примера за один шаг обновления весов.​
Регуляризация
Dropout(0.3) – на каждом шаге случайно отключается 30% нейронов скрытых слоёв, чтобы модель не «зубрила» тренажёрные данные и лучше обобщала.​
EarlyStopping(patience=20) – обучение останавливается, если валидационная ошибка не улучшается 20 эпох подряд; при этом восстанавливаются лучшие веса. На практике модель стабилизируется примерно к 80–100‑й эпохе, хотя верхняя граница задана 150.​

2.3. Описание датасета
Источник: Kaggle, датасет Customer Purchasing Behaviors.​
Файл: shopping_trends.csv.
Объём: около 3900 примеров (в презентации округлённо указано 3500).​
Признаки (feature‑ы), всего 18:
Категориальные (строковые, далее кодируются в числа):
Gender – пол клиента.
Category – категория товара.
Size – размер.
Color – цвет.
Season – сезон.
Subscription Status – наличие подписки и др.​
Числовые:
Age – возраст.
Review Rating – рейтинг товара.
Previous Purchases – количество предыдущих покупок и другие количественные поля.​
Целевая переменная (Target):
Purchase Amount (USD) – сумма покупки в долларах.
Диапазон: примерно от 20 до 100 USD, среднее значение около 60.1 USD.​
Разделение выборки и предобработка:
Данные делятся на 80% обучающая выборка и 20% тестовая – модель учится на 80% и проверяется на оставшихся 20% клиентов.​
Для категориальных признаков используется OrdinalEncoder – переводит текст в порядковые числа.
Для числовых признаков применяется StandardScaler – стандартизация вида (x−mean)/std, чтобы разные признаки имели сопоставимый масштаб.​

2.4. Метрики и результаты 
Для оценки качества использовались три основные метрики регрессии:​
R² (коэффициент детерминации): 0.8234.
Это означает, что модель объясняет около 82% разброса в значениях суммы покупки на тестовой выборке.
MAE (Mean Absolute Error): 3.27 USD.
Средняя ошибка по модулю – модель в среднем ошибается примерно на 3 доллара относительно реальной суммы чека.​
RMSE (Root Mean Squared Error): 12.97 USD.
Это корень из MSE, показывающий ошибку в тех же единицах, что и целевая переменная.​
По графикам обучения (loss/val_loss, MAE/val_MAE):
Ошибка на обучающей и валидационной выборке плавно уменьшается и выходит на плато без резкого расхождения кривых.
Это говорит о хорошей обобщающей способности и отсутствии серьёзного переобучения.​

2.5. Демонстрация работы нейросети 
В финальной версии проекта модель может быть развёрнута в виде простого веб‑интерфейса (например, через Gradio или аналогичный инструмент):​
Пользователь вводит параметры клиента: пол, возраст, категорию товара, сезон, наличие подписки и т.п.
Интерфейс под капотом применяет тот же препроцессинг (OrdinalEncoder + StandardScaler) и передаёт данные в обученную FCN.
Модель возвращает прогнозируемую сумму покупки в долларах, который сразу отображается на странице.​
Такой формат удобно использовать и для демонстрации на защите, и для последующей интеграции в бизнес‑процесс.

2.6. Пять ключевых алгоритмов
На практике в проекте задействовано пять базовых алгоритмов/подходов:​
OrdinalEncoder – переводит текстовые значения (пол, сезон, категория) в числа, чтобы нейросеть могла с ними работать.
StandardScaler – нормализует числовые признаки, чтобы все они были примерно в одном масштабе.
Adam Optimizer – адаптивный алгоритм оптимизации, который на каждом шаге обучения подбирает удобный шаг изменения весов.
Backpropagation + ReLU – связка основного алгоритма обучения (обратное распространение ошибки) и функции активации ReLU в скрытых слоях.
Dropout + EarlyStopping – два приёма регуляризации: случайное отключение части нейронов и ранняя остановка обучения при отсутствии улучшения на валидации.​
Все пять подходов реально используются в коде обучения модели, а не только описаны теоретически.

2.7. Дальнейшая доработка и репозиторий 
В качестве следующих шагов по развитию проекта планируются:​
Добавление рекуррентных архитектур (например, LSTM) для учёта временных паттернов, если появятся данные по истории покупок во времени.
Интеграция модели с CRM‑системами и рекомендательными сервисами интернет‑магазина.
Развёртывание в облаке (например, AWS или Google Cloud) для использования в реальном продукте.​
​
